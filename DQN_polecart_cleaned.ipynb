{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Gradient vs. Full-Gradient #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import gym\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "          out=self.l1(x)\n",
    "          out=F.relu(out)\n",
    "          out=self.l2(out)\n",
    "        \n",
    "          return out\n",
    "        \n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory)>self.capacity:\n",
    "            self.memory=self.memory[1:]\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def get_epsilon(it):\n",
    "    epsilon =np.maximum(1+ - .95*((it)/1000),.05)\n",
    "    \n",
    "    return epsilon\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "        Args:\n",
    "            obs: current state\n",
    "        Returns:\n",
    "            An action (int).\n",
    "            \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "            q=self.Q(obs)\n",
    "            p=random.random()\n",
    "            if p<self.epsilon:\n",
    "                return   int(len(q) * p/self.epsilon)\n",
    "            else:\n",
    "                return np.argmax(q).item()\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "def compute_q_vals(Q, states, actions):\n",
    "    try:\n",
    "        return torch.gather(Q.forward(states),1, actions)\n",
    "    except:\n",
    "        return Q_net.forward(state)[action].flatten()\n",
    "\n",
    "    \n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "\n",
    "    dones=dones.squeeze()\n",
    "    ndmask=(1-dones.type(torch.FloatTensor))\n",
    "    \n",
    "    targets= rewards.squeeze()+(discount_factor*torch.max(Q(next_states),1)[0])*ndmask.flatten()\n",
    "\n",
    "    return targets.reshape(len(dones),1)\n",
    "\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor,semi):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    loss_func = nn.MSELoss()\n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    \n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    if semi==True:\n",
    "        with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "            target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    else:\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "\n",
    "    loss = loss_func(q_val, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi):\n",
    "#     print(episode_durations)\n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "\n",
    "  \n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = [] \n",
    "    global_iter = 0\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps=steps+1\n",
    "            epsilon=get_epsilon(global_steps)\n",
    "            policy.set_epsilon(epsilon)\n",
    "            a = policy.sample_action(state)\n",
    "            ns, reward, done, _ = env.step(a)\n",
    "            memory.push((state, a, reward, ns, done))\n",
    "            train(Q, memory, optimizer, batch_size, discount_factor,semi)\n",
    "            state=ns\n",
    "            global_steps=global_steps+1\n",
    "            if done:\n",
    "                if i % 100 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    #print(episode_durations)\n",
    "  \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0 finished after 21 steps\n",
      " Episode 100 finished after 124 steps\n",
      " Episode 200 finished after 145 steps\n",
      " Episode 300 finished after 500 steps\n",
      " Episode 400 finished after 18 steps\n",
      " Episode 500 finished after 369 steps\n",
      " Episode 600 finished after 231 steps\n",
      " Episode 700 finished after 271 steps\n",
      " Episode 800 finished after 10 steps\n",
      " Episode 900 finished after 500 steps\n",
      "0 0:09:03.406224\n",
      " Episode 0 finished after 21 steps\n",
      " Episode 100 finished after 48 steps\n",
      " Episode 200 finished after 9 steps\n",
      " Episode 300 finished after 415 steps\n",
      " Episode 400 finished after 9 steps\n",
      " Episode 500 finished after 11 steps\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "results={}\n",
    "results2={}\n",
    "counter=0\n",
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "\n",
    "x=datetime.datetime.now()\n",
    "for semib in [True, False]:\n",
    "    for seed in range(10):\n",
    "        for epsilon in [0.0]:\n",
    "            for discount_factor in [1]:\n",
    "                # We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "                random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                env.seed(seed)\n",
    "\n",
    "                Q_net = QNetwork(num_hidden)\n",
    "                policy = EpsilonGreedyPolicy(Q_net, epsilon)\n",
    "                episode_durations1 = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate,semi=semib)\n",
    "                \n",
    "                results[counter]={'EpisodeDuration':episode_durations1,'Seed':seed,'SemiB':semib,'Epsilon':epsilon,'DisFactor':discount_factor}\n",
    "                #results2[counter]={'DQN':Q_net,'EpisodeDuration':episode_durations1,'Seed':seed,'SemiB':semib,'Epsilon':epsilon,'DisFactor':discount_factor}\n",
    "\n",
    "                counter=counter+1\n",
    "        print(seed, datetime.datetime.now()-x)\n",
    "    results_df=pd.DataFrame(results).T\n",
    "    results_df.to_json('results_semi_{}.json'.format(semib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(10):\n",
    "    tmp.append(results_df.loc[i, 'EpisodeDuration'])\n",
    "durations_np = np.array(tmp)\n",
    "#mean = np.array(results11[0])\n",
    "print(durations_np.shape)\n",
    "mean = durations_np.mean(axis=0)\n",
    "std = durations_np.std(axis=0)\n",
    "print(mean.shape, std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(range(len(mean)), mean, '-', color='blue')\n",
    "\n",
    "plt.fill_between(range(len(mean)), mean-std, mean+std,\n",
    "                 color='blue', alpha=0.2)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Duration (ms)')\n",
    "plt.title('Semi-gradient Performance on Cart Pole\\n(Mean and Std)')\n",
    "plt.show()\n",
    "#plt.savefig('cart_pole_semi.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
