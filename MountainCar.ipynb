{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Deep Q Network\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dqn_autograde.py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\howar\\anaconda3\\envs\\rl2020\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1)\n",
    "Q_net = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(Q_net(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to stabilize learning, we will use an experience replay to save states in and sample states from. Now implement the `push` function that adds a transition to the replay buffer, and the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`). It should keep at most the maximum number of transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.5036109,  0.       ]),\n",
       " array([-0.41584584,  0.        ]),\n",
       " array([-0.50313229,  0.        ])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in memory.sample(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.50313229,  0.        ]), 0, -1.0, array([-0.50428569, -0.0011534 ]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "# memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function of *EpsilonGreedyPolicy* class. This function takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon. Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "epg = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "a = epg.sample_action(s)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def compute_q_vals(Q, states, actions):\n",
    "    \"\"\"\n",
    "    This method returns Q values for given state action pairs.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        actions: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "\n",
    "    Returns:\n",
    "        A torch tensor filled with Q values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-addefbd97148>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Now let's see if it works\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-941e223902bc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Q, memory, optimizer, batch_size, discount_factor)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# convert to PyTorch and define types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(Q_net, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        steps = 0\n",
    "        while True:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            if done:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(2, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 3)\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "#         raise NotImplementedError\n",
    "          out=self.l1(x)\n",
    "          out=F.relu(out)\n",
    "          out=self.l2(out)\n",
    "        \n",
    "          return out\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    def push(self, transition):\n",
    "        # YOUR CODE HERE\n",
    "#         raise NotImplemente\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory)>self.capacity:\n",
    "            self.memory=self.memory[1:]\n",
    "    def sample(self, batch_size):\n",
    "        # YOUR CODE HERE\n",
    "#         raise NotImplementedError\n",
    "        return random.sample(self.memory,batch_size)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError\n",
    "    \n",
    "    epsilon =np.maximum(1+ - .95*((it)/1000),.05)\n",
    "    \n",
    "    return epsilon\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "        Args:\n",
    "            obs: current state\n",
    "        Returns:\n",
    "            An action (int).\n",
    "            \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "#         raise NotImplementedError\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "            q=self.Q(obs)\n",
    "            p=random.random()\n",
    "#             print(p)\n",
    "            if p<self.epsilon:\n",
    "                return   int(len(q) * p/self.epsilon)\n",
    "#             random.rand(),1)[0]\n",
    "            else:\n",
    "#                 print(self.Q(obs))\n",
    "                return np.argmax(q).item()\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "def compute_q_vals(Q, states, actions):\n",
    "\n",
    "    try:\n",
    "        return torch.gather(Q.forward(states),1, actions)\n",
    "    except:\n",
    "        return Q_net.forward(state)[action].flatten()\n",
    "\n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "\n",
    "    dones=dones.squeeze()\n",
    "    ndmask=(1-dones.type(torch.FloatTensor))\n",
    "    \n",
    "    targets= rewards.squeeze()+(discount_factor*torch.max(Q(next_states),1)[0])*ndmask.flatten()\n",
    "\n",
    "    return targets.reshape(len(dones),1)\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor,semi):\n",
    "    loss_func = nn.MSELoss()\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    \n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "    \n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    if semi==True:\n",
    "        with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "            target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    else:\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "\n",
    "\n",
    "    loss = loss_func(q_val, target)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate,semi):\n",
    "#     print(episode_durations)\n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "\n",
    "  \n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = [] \n",
    "    global_iter = 0\n",
    "    for i in range(num_episodes):\n",
    "        max_state=-.5\n",
    "        state = env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # YOUR CODE HERE\n",
    "    #             raise NotImplementedError\n",
    "            steps=steps+1\n",
    "            epsilon=get_epsilon(global_steps)\n",
    "            policy.set_epsilon(epsilon)\n",
    "    #             epg = EpsilonGreedyPolicy(Q, epsilon)\n",
    "            a = policy.sample_action(state)\n",
    "            ns, reward, done, _ = env.step(a)\n",
    "            if state[0]>max_state:\n",
    "                max_state=state[0]\n",
    "                reward=reward+0.5\n",
    "            elif state[0]>0.5:\n",
    "                reward=reward+10\n",
    "    #             print(done)\n",
    "            memory.push((state, a, reward, ns, done))\n",
    "            # if len(memory)>batch_size*3:\n",
    "#                 for i in range(np.maximum(int(batch_size/episode_durations[-1]),5)):\n",
    "            train(Q, memory, optimizer, batch_size, discount_factor,semi)\n",
    "    \n",
    "            state=ns\n",
    "#             print(ns)\n",
    "            global_steps=global_steps+1\n",
    "#             print(reward)\n",
    "            if done:\n",
    "                if state[0]>=.5:\n",
    "                    epsilon=epsilon/10.\n",
    "                if i % 10 == 0:\n",
    "                    print(state)\n",
    "                    print(reward)\n",
    "\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append((steps,state[0]))\n",
    "                #plot_durations()\n",
    "                break\n",
    "    print(episode_durations)\n",
    "  \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.54270599  0.00074552]\n",
      "-1.0\n",
      " Episode 0 finished after 200 steps\n",
      "[-0.28398293  0.00171346]\n",
      "-0.5\n",
      " Episode 10 finished after 200 steps\n",
      "[-0.74293878  0.01990699]\n",
      "-1.0\n",
      " Episode 20 finished after 200 steps\n",
      "[-0.38686089  0.02426046]\n",
      "-1.0\n",
      " Episode 30 finished after 200 steps\n",
      "[-0.7421389   0.00798117]\n",
      "-1.0\n",
      " Episode 40 finished after 200 steps\n",
      "[-0.54463779 -0.0280715 ]\n",
      "-1.0\n",
      " Episode 50 finished after 200 steps\n",
      "[-1.0359608  -0.02716854]\n",
      "-1.0\n",
      " Episode 60 finished after 200 steps\n",
      "[-0.36339155  0.04811267]\n",
      "-1.0\n",
      " Episode 70 finished after 200 steps\n",
      "[-0.98947505 -0.01868918]\n",
      "-1.0\n",
      " Episode 80 finished after 200 steps\n",
      "[0.1615216  0.02892735]\n",
      "-0.5\n",
      " Episode 90 finished after 200 steps\n",
      "[-0.35292802  0.01270055]\n",
      "-0.5\n",
      " Episode 100 finished after 200 steps\n",
      "[-0.22460637  0.02047845]\n",
      "-0.5\n",
      " Episode 110 finished after 200 steps\n",
      "[-0.59739353 -0.00540613]\n",
      "-1.0\n",
      " Episode 120 finished after 200 steps\n",
      "[-0.26374313  0.00871016]\n",
      "-0.5\n",
      " Episode 130 finished after 200 steps\n",
      "[-0.79526548 -0.01012954]\n",
      "-1.0\n",
      " Episode 140 finished after 200 steps\n",
      "[-0.53906157  0.00113859]\n",
      "-1.0\n",
      " Episode 150 finished after 200 steps\n",
      "[-0.61252143  0.0045473 ]\n",
      "-1.0\n",
      " Episode 160 finished after 200 steps\n",
      "[-0.78288388 -0.0152503 ]\n",
      "-1.0\n",
      " Episode 170 finished after 200 steps\n",
      "[-0.63912943 -0.00064284]\n",
      "-1.0\n",
      " Episode 180 finished after 200 steps\n",
      "[-0.62875645 -0.00152638]\n",
      "-1.0\n",
      " Episode 190 finished after 200 steps\n",
      "[-0.50411916 -0.00829661]\n",
      "-1.0\n",
      " Episode 200 finished after 200 steps\n",
      "[0.21794795 0.01222039]\n",
      "-0.5\n",
      " Episode 210 finished after 200 steps\n",
      "[0.52717583 0.0290051 ]\n",
      "-0.5\n",
      " Episode 220 finished after 188 steps\n",
      "[0.50702875 0.01343127]\n",
      "-0.5\n",
      " Episode 230 finished after 179 steps\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-6a74c7f47995>\u001b[0m in \u001b[0;36mcompute_q_vals\u001b[1;34m(Q, states, actions)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-6a74c7f47995>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#         raise NotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m           \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m           \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rl2020\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rl2020\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rl2020\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1674\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-6d11504d1fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mQ_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEpsilonGreedyPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mepisode_durations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msemi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-6a74c7f47995>\u001b[0m in \u001b[0;36mrun_episodes\u001b[1;34m(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# if len(memory)>batch_size*3:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;31m#                 for i in range(np.maximum(int(batch_size/episode_durations[-1]),5)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msemi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-6a74c7f47995>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Q, memory, optimizer, batch_size, discount_factor, semi)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Boolean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mq_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_q_vals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msemi\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Don't compute gradient info for the target (semi-gradient)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-6a74c7f47995>\u001b[0m in \u001b[0;36mcompute_q_vals\u001b[1;34m(Q, states, actions)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mQ_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 400\n",
    "batch_size = 64\n",
    "discount_factor = 0.999\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.3)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate,semi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0 finished after 17 steps\n",
      " Episode 10 finished after 28 steps\n",
      " Episode 20 finished after 16 steps\n",
      " Episode 30 finished after 12 steps\n",
      " Episode 40 finished after 13 steps\n",
      " Episode 50 finished after 72 steps\n",
      " Episode 60 finished after 129 steps\n",
      " Episode 70 finished after 136 steps\n",
      " Episode 80 finished after 153 steps\n",
      " Episode 90 finished after 148 steps\n",
      "[17, 16, 26, 13, 19, 29, 21, 32, 24, 25, 28, 11, 15, 19, 12, 9, 18, 13, 10, 15, 16, 11, 15, 10, 11, 11, 16, 12, 20, 12, 12, 21, 12, 69, 32, 15, 22, 49, 26, 37, 13, 55, 54, 61, 75, 85, 97, 118, 82, 76, 72, 105, 81, 112, 102, 109, 157, 106, 121, 95, 129, 159, 179, 447, 110, 198, 107, 178, 200, 143, 136, 198, 135, 156, 134, 177, 246, 139, 148, 128, 153, 150, 134, 184, 131, 164, 165, 151, 145, 155, 148, 127, 172, 169, 186, 141, 140, 203, 191, 142]\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.13235725e-01  3.43684633e-04]\n",
      "-1.0\n",
      " Episode 0 finished after 200 steps\n",
      "[-0.42463955  0.00980025]\n",
      "-1.0\n",
      " Episode 10 finished after 200 steps\n",
      "[-0.29778098  0.00680093]\n",
      "-0.5\n",
      " Episode 20 finished after 200 steps\n",
      "[-0.38873626  0.02462561]\n",
      "-1.0\n",
      " Episode 30 finished after 200 steps\n",
      "[-0.35720213  0.02791976]\n",
      "-1.0\n",
      " Episode 40 finished after 200 steps\n",
      "[-0.81305431  0.03295228]\n",
      "-1.0\n",
      " Episode 50 finished after 200 steps\n",
      "[-0.46216389 -0.04074988]\n",
      "-1.0\n",
      " Episode 60 finished after 200 steps\n",
      "[-0.216681    0.02141441]\n",
      "-1.0\n",
      " Episode 70 finished after 200 steps\n",
      "[-0.20903147  0.02804386]\n",
      "-1.0\n",
      " Episode 80 finished after 200 steps\n",
      "[-0.3073842   0.03194309]\n",
      "-1.0\n",
      " Episode 90 finished after 200 steps\n",
      "[-0.49949843  0.03997873]\n",
      "-1.0\n",
      " Episode 100 finished after 200 steps\n",
      "[-0.08289499  0.02268688]\n",
      "-1.0\n",
      " Episode 110 finished after 200 steps\n",
      "[-0.21430337  0.0232283 ]\n",
      "-1.0\n",
      " Episode 120 finished after 200 steps\n",
      "[-0.18067612  0.02589046]\n",
      "-1.0\n",
      " Episode 130 finished after 200 steps\n",
      "[0.12386532 0.01157386]\n",
      "-0.5\n",
      " Episode 140 finished after 200 steps\n",
      "[-0.06864293  0.02270436]\n",
      "-1.0\n",
      " Episode 150 finished after 200 steps\n",
      "[-0.21014542  0.03594584]\n",
      "-1.0\n",
      " Episode 160 finished after 200 steps\n",
      "[-0.03174843  0.01423392]\n",
      "-0.5\n",
      " Episode 170 finished after 200 steps\n",
      "[-0.12461205  0.00796305]\n",
      "-0.5\n",
      " Episode 180 finished after 200 steps\n",
      "[-0.47271664  0.03596455]\n",
      "-1.0\n",
      " Episode 190 finished after 200 steps\n",
      "[-0.4918185   0.04288664]\n",
      "-1.0\n",
      " Episode 200 finished after 200 steps\n",
      "[-0.90629818 -0.0191753 ]\n",
      "-1.0\n",
      " Episode 210 finished after 200 steps\n",
      "[-0.25773576  0.03322231]\n",
      "-1.0\n",
      " Episode 220 finished after 200 steps\n",
      "[-1.00947054  0.01707322]\n",
      "-1.0\n",
      " Episode 230 finished after 200 steps\n",
      "[-0.12585771  0.02069607]\n",
      "-0.5\n",
      " Episode 240 finished after 200 steps\n",
      "[-0.52341138  0.03622413]\n",
      "-1.0\n",
      " Episode 250 finished after 200 steps\n",
      "[-0.34738224  0.03772771]\n",
      "-1.0\n",
      " Episode 260 finished after 200 steps\n",
      "[-0.66515309  0.01286375]\n",
      "-1.0\n",
      " Episode 270 finished after 200 steps\n",
      "[-0.30360146  0.03258128]\n",
      "-1.0\n",
      " Episode 280 finished after 200 steps\n",
      "[-0.32768359  0.02249622]\n",
      "-1.0\n",
      " Episode 290 finished after 200 steps\n",
      "[-0.23154102  0.02951961]\n",
      "-1.0\n",
      " Episode 300 finished after 200 steps\n",
      "[-0.61273655  0.03831074]\n",
      "-1.0\n",
      " Episode 310 finished after 200 steps\n",
      "[-0.63808024  0.04081645]\n",
      "-1.0\n",
      " Episode 320 finished after 200 steps\n",
      "[-0.33907334  0.03108576]\n",
      "-1.0\n",
      " Episode 330 finished after 200 steps\n",
      "[-0.90829647 -0.02469607]\n",
      "-1.0\n",
      " Episode 340 finished after 200 steps\n",
      "[-0.91578225  0.01663942]\n",
      "-1.0\n",
      " Episode 350 finished after 200 steps\n",
      "[-0.20696594  0.01574914]\n",
      "-1.0\n",
      " Episode 360 finished after 200 steps\n",
      "[-0.25050621  0.01575677]\n",
      "-1.0\n",
      " Episode 370 finished after 200 steps\n",
      "[-0.51398337  0.02175924]\n",
      "-1.0\n",
      " Episode 380 finished after 200 steps\n",
      "[-0.34008313  0.01840415]\n",
      "-1.0\n",
      " Episode 390 finished after 200 steps\n",
      "[-0.34674745  0.01573736]\n",
      "-1.0\n",
      " Episode 400 finished after 200 steps\n",
      "[-0.26596021  0.01115141]\n",
      "-1.0\n",
      " Episode 410 finished after 200 steps\n",
      "[-0.20417069  0.00991959]\n",
      "-0.5\n",
      " Episode 420 finished after 200 steps\n",
      "[-0.17687449  0.01638216]\n",
      "-0.5\n",
      " Episode 430 finished after 200 steps\n",
      "[-0.16785083  0.01458988]\n",
      "-0.5\n",
      " Episode 440 finished after 200 steps\n",
      "[-0.20984358  0.00524144]\n",
      "-1.0\n",
      " Episode 450 finished after 200 steps\n",
      "[-0.13234832  0.01571539]\n",
      "-0.5\n",
      " Episode 460 finished after 200 steps\n",
      "[-0.11358249  0.01007591]\n",
      "-0.5\n",
      " Episode 470 finished after 200 steps\n",
      "[-0.3726034   0.02732297]\n",
      "-1.0\n",
      " Episode 480 finished after 200 steps\n",
      "[-0.18537105  0.0021938 ]\n",
      "-1.0\n",
      " Episode 490 finished after 200 steps\n",
      "[-0.61846629 -0.02138029]\n",
      "-1.0\n",
      " Episode 500 finished after 200 steps\n",
      "[-0.50162     0.01614684]\n",
      "-1.0\n",
      " Episode 510 finished after 200 steps\n",
      "[-0.24015283  0.01014743]\n",
      "-1.0\n",
      " Episode 520 finished after 200 steps\n",
      "[-0.38483941  0.01748277]\n",
      "-1.0\n",
      " Episode 530 finished after 200 steps\n",
      "[-8.85880143e-01  2.58940425e-04]\n",
      "-1.0\n",
      " Episode 540 finished after 200 steps\n",
      "[-0.14788482  0.01075852]\n",
      "-0.5\n",
      " Episode 550 finished after 200 steps\n",
      "[-0.23463003 -0.02537214]\n",
      "-1.0\n",
      " Episode 560 finished after 200 steps\n",
      "[-0.23112528 -0.02104821]\n",
      "-1.0\n",
      " Episode 570 finished after 200 steps\n",
      "[-0.86797247 -0.00919858]\n",
      "-1.0\n",
      " Episode 580 finished after 200 steps\n",
      "[-8.97654345e-01  5.34590113e-04]\n",
      "-1.0\n",
      " Episode 590 finished after 200 steps\n",
      "[-0.09847704 -0.00627016]\n",
      "-1.0\n",
      " Episode 600 finished after 200 steps\n",
      "[-0.20779229  0.0085346 ]\n",
      "-0.5\n",
      " Episode 610 finished after 200 steps\n",
      "[-0.31830992  0.0249326 ]\n",
      "-1.0\n",
      " Episode 620 finished after 200 steps\n",
      "[-0.13849223 -0.00100874]\n",
      "-0.5\n",
      " Episode 630 finished after 200 steps\n",
      "[-0.69474851  0.03053306]\n",
      "-1.0\n",
      " Episode 640 finished after 200 steps\n",
      "[-0.21419975  0.00305994]\n",
      "-1.0\n",
      " Episode 650 finished after 200 steps\n",
      "[-0.40340447  0.01513432]\n",
      "-1.0\n",
      " Episode 660 finished after 200 steps\n",
      "[-0.41216275  0.02310466]\n",
      "-1.0\n",
      " Episode 670 finished after 200 steps\n",
      "[-0.51508023  0.03904024]\n",
      "-1.0\n",
      " Episode 680 finished after 200 steps\n",
      "[-0.24642497  0.01107048]\n",
      "-1.0\n",
      " Episode 690 finished after 200 steps\n",
      "[-0.09036796 -0.00011442]\n",
      "-0.5\n",
      " Episode 700 finished after 200 steps\n",
      "[-0.54059467  0.0391246 ]\n",
      "-1.0\n",
      " Episode 710 finished after 200 steps\n",
      "[-0.16730753  0.02304969]\n",
      "-0.5\n",
      " Episode 720 finished after 200 steps\n",
      "[-0.07832162 -0.0018558 ]\n",
      "-1.0\n",
      " Episode 730 finished after 200 steps\n",
      "[-0.01246566  0.00480168]\n",
      "-0.5\n",
      " Episode 740 finished after 200 steps\n",
      "[-0.25992732  0.02413551]\n",
      "-1.0\n",
      " Episode 750 finished after 200 steps\n",
      "[-0.7565576   0.01406338]\n",
      "-1.0\n",
      " Episode 760 finished after 200 steps\n",
      "[-0.12318846  0.00209871]\n",
      "-0.5\n",
      " Episode 770 finished after 200 steps\n",
      "[-0.5533969   0.00972885]\n",
      "-1.0\n",
      " Episode 780 finished after 200 steps\n",
      "[-0.36285714  0.01960661]\n",
      "-1.0\n",
      " Episode 790 finished after 200 steps\n",
      "[-0.12131828  0.00250821]\n",
      "-0.5\n",
      " Episode 800 finished after 200 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-3efdc6237f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mQ_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEpsilonGreedyPolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mepisode_durations1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msemi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msemib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'EpisodeDuration'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mepisode_durations1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Seed'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'SemiB'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msemib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Epsilon'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DisFactor'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-312218235da7>\u001b[0m in \u001b[0;36mrun_episodes\u001b[1;34m(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi)\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;31m# if len(memory)>batch_size*3:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;31m#                 for i in range(np.maximum(int(batch_size/episode_durations[-1]),5)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msemi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-312218235da7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Q, memory, optimizer, batch_size, discount_factor, semi)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Returns a Python scalar, and releases history (similar to .detach())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rl2020\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rl2020\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen#\n",
    "results={}\n",
    "results2={}\n",
    "counter=0\n",
    "import datetime as datetime\n",
    "x=datetime.datetime.now()\n",
    "for seed in range(10):\n",
    "    for semib in [True,False]:\n",
    "        for epsilon in [0.0,.01,.05,.3]:\n",
    "            for discount_factor in [0.5,.8,1]:\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "                random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                env.seed(seed)\n",
    "\n",
    "                Q_net = QNetwork(num_hidden)\n",
    "                policy = EpsilonGreedyPolicy(Q_net, epsilon)\n",
    "                episode_durations1 = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate,semi=semib)\n",
    "                \n",
    "                results[counter]={'EpisodeDuration':episode_durations1,'Seed':seed,'SemiB':semib,'Epsilon':epsilon,'DisFactor':discount_factor}\n",
    "                results2[counter]={'DQN':Q_net,'EpisodeDuration':episode_durations1,'Seed':seed,'SemiB':semib,'Epsilon':epsilon,'DisFactor':discount_factor}\n",
    "\n",
    "                counter=counter+1\n",
    "    print(datetime.datetime.now()-x)\n",
    "results11=pd.DataFrame(results).T\n",
    "results11.to_hdf('results.h5','df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
