{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Deep Q Network\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dqn_autograde.py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl2020/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"Acrobot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "# obs = env.reset()\n",
    "# env.render()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     time.sleep(0.05)\n",
    "# env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(6, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        out = self.l1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4707, 0.7595, 0.5687, 0.8585, 0.6079, 0.4938],\n",
      "        [0.5709, 0.8333, 0.5755, 0.8522, 0.4526, 0.5043],\n",
      "        [0.9201, 0.0280, 0.5499, 0.9567, 0.2491, 0.4078],\n",
      "        [0.4699, 0.6647, 0.6890, 0.6133, 0.6972, 0.2855],\n",
      "        [0.7104, 0.3415, 0.8142, 0.3533, 0.0742, 0.5357],\n",
      "        [0.7602, 0.4804, 0.5065, 0.5018, 0.7654, 0.5391],\n",
      "        [0.0533, 0.1434, 0.5296, 0.4599, 0.9779, 0.3825],\n",
      "        [0.8827, 0.0783, 0.0248, 0.4540, 0.1834, 0.3542],\n",
      "        [0.5563, 0.0663, 0.8436, 0.1966, 0.8836, 0.6634],\n",
      "        [0.4062, 0.0624, 0.5828, 0.2207, 0.9321, 0.0794]])\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1)\n",
    "Q_net = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(6, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 3)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 6)\n",
    "print(x)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(Q_net(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to stabilize learning, we will use an experience replay to save states in and sample states from. Now implement the `push` function that adds a transition to the replay buffer, and the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`). It should keep at most the maximum number of transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        \n",
    "    def push(self, transition):\n",
    "        # YOUR CODE HERE\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            # remove first entry\n",
    "            self.memory.pop(0)\n",
    "        # fill memory\n",
    "        self.memory.append(transition)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # YOUR CODE HERE\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "[(array([ 0.99984796, -0.01743732,  0.99885355,  0.0478705 ,  0.03982028,\n",
      "        0.05807792]), 0, -1.0, array([ 0.99997871,  0.00652519,  0.99982819,  0.0185364 ,  0.19382255,\n",
      "       -0.3424237 ]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print('----')\n",
    "print(memory.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def get_epsilon(it):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # prob of taking greedy action\n",
    "    greedy = 0.0\n",
    "    if it > 1000:\n",
    "        # greedy capped at 0.95\n",
    "        greedy = 0.95\n",
    "    else:\n",
    "        # interpolation betw. 0.0 and 0.95\n",
    "        greedy = it * 0.95 / 1000\n",
    "    \n",
    "    epsilon = 1.0 - greedy\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd600b1fe10>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function of *EpsilonGreedyPolicy* class. This function takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon. Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q.float()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        assert self.epsilon <= 1.0, 'epsilon>1 is not a valid probability!'\n",
    "        policy_choice = np.random.choice(('greedy','non-greedy'), p=[1.0-self.epsilon, self.epsilon])\n",
    "        \n",
    "        if policy_choice == 'greedy':\n",
    "            with torch.no_grad():\n",
    "#                 print(\"before logits: \",torch.tensor(obs).float() )\n",
    "                \n",
    "                logits = self.Q(torch.tensor(obs).float())\n",
    "#                 print(\"LOGITS: \",logits )\n",
    "                action = torch.argmax(logits).item()\n",
    "        elif policy_choice == 'non-greedy':\n",
    "            action = random.choice([0,1])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        return action\n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "epg = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "a = epg.sample_action(s)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def compute_q_vals(Q, states, actions):\n",
    "    \"\"\"\n",
    "    This method returns Q values for given state action pairs.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        actions: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "\n",
    "    Returns:\n",
    "        A torch tensor filled with Q values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "#     print(\"STATES shape IS: \", states.shape)\n",
    "    qvals = Q(states)\n",
    "    q_sa = qvals.gather(1, actions)\n",
    "    return q_sa\n",
    "    \n",
    "    \n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if dones.dtype != torch.uint8:\n",
    "        dones = torch.tensor(dones, dtype=torch.uint8)\n",
    "    q_sp = Q(next_states)\n",
    "    maxq, _ = q_sp.max(dim=1, keepdim=True)\n",
    "    targets = rewards + discount_factor * maxq * (1 - dones)\n",
    "    return targets\n",
    "\n",
    "\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor, semi):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "#     print(\"state is : \", state)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "\n",
    "    # compute the q value\n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    if semi==True:\n",
    "        with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "            target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    else:\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2648017108440399\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(Q_net, memory, optimizer, batch_size, discount_factor, False)\n",
    "\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi):\n",
    "    \n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        steps = 0\n",
    "        while True:\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # get epsilon update\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            # update epsilon\n",
    "            policy.set_epsilon(epsilon)\n",
    "            # increment steps\n",
    "            global_steps += 1\n",
    "            steps += 1\n",
    "            \n",
    "            # sample action and store in memory\n",
    "            action = policy.sample_action(state)\n",
    "            s_next, reward, done, _ = env.step(action)\n",
    "            memory.push((state, action, reward, s_next, done))\n",
    "            loss = train(Q, memory, optimizer, batch_size, discount_factor, semi)\n",
    "            state = s_next\n",
    "            \n",
    "#             if loss != None:\n",
    "#                 print(loss)\n",
    "            \n",
    "            if done:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 500 steps\n",
      "\u001b[92m Episode 20 finished after 209 steps\n",
      "\u001b[92m Episode 30 finished after 264 steps\n",
      "\u001b[92m Episode 40 finished after 336 steps\n",
      "it took  33.08741617202759  seconds to run the program\n"
     ]
    }
   ],
   "source": [
    "# # Let's run it!\n",
    "# num_episodes = 50\n",
    "# batch_size = 64\n",
    "# discount_factor = 0.8\n",
    "# learn_rate = 1e-3\n",
    "# memory = ReplayMemory(10000)\n",
    "# num_hidden = 128\n",
    "# seed = 42  # This is not randomly chosen\n",
    "\n",
    "# # We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "# random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# env.seed(seed)\n",
    "\n",
    "# semi_grad = False\n",
    "\n",
    "# Q_net = QNetwork(num_hidden)\n",
    "# policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "# start = time.time()\n",
    "# episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi_grad)\n",
    "# end = time.time()\n",
    "# a = np.trasnpose(np.asarray(episode_durations))\n",
    "# np.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "# print(\"it took \", end - start, \" seconds to run the program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 332 steps\n",
      "\u001b[99m Episode 20 finished after 140 steps\n",
      "\u001b[92m Episode 30 finished after 337 steps\n",
      "\u001b[99m Episode 40 finished after 183 steps\n",
      "\u001b[92m Episode 50 finished after 207 steps\n",
      "\u001b[92m Episode 60 finished after 230 steps\n",
      "\u001b[92m Episode 70 finished after 315 steps\n",
      "\u001b[92m Episode 80 finished after 323 steps\n",
      "\u001b[92m Episode 90 finished after 438 steps\n",
      "\u001b[92m Episode 100 finished after 352 steps\n",
      "\u001b[99m Episode 110 finished after 173 steps\n",
      "\u001b[92m Episode 120 finished after 274 steps\n",
      "\u001b[92m Episode 130 finished after 269 steps\n",
      "\u001b[92m Episode 140 finished after 290 steps\n",
      "\u001b[99m Episode 150 finished after 145 steps\n",
      "\u001b[92m Episode 160 finished after 294 steps\n",
      "\u001b[99m Episode 170 finished after 150 steps\n",
      "\u001b[92m Episode 180 finished after 226 steps\n",
      "\u001b[92m Episode 190 finished after 212 steps\n",
      "\u001b[92m Episode 200 finished after 214 steps\n",
      "\u001b[92m Episode 210 finished after 242 steps\n",
      "\u001b[92m Episode 220 finished after 224 steps\n",
      "\u001b[92m Episode 230 finished after 198 steps\n",
      "\u001b[92m Episode 240 finished after 213 steps\n",
      "\u001b[99m Episode 250 finished after 160 steps\n",
      "\u001b[92m Episode 260 finished after 210 steps\n",
      "\u001b[92m Episode 270 finished after 308 steps\n",
      "\u001b[92m Episode 280 finished after 440 steps\n",
      "\u001b[99m Episode 290 finished after 184 steps\n",
      "\u001b[92m Episode 300 finished after 222 steps\n",
      "\u001b[92m Episode 310 finished after 317 steps\n",
      "\u001b[92m Episode 320 finished after 333 steps\n",
      "\u001b[99m Episode 330 finished after 177 steps\n",
      "\u001b[99m Episode 340 finished after 165 steps\n",
      "\u001b[92m Episode 350 finished after 232 steps\n",
      "\u001b[92m Episode 360 finished after 218 steps\n",
      "\u001b[92m Episode 370 finished after 303 steps\n",
      "\u001b[99m Episode 380 finished after 160 steps\n",
      "\u001b[99m Episode 390 finished after 188 steps\n",
      "\u001b[92m Episode 400 finished after 237 steps\n",
      "\u001b[92m Episode 410 finished after 205 steps\n",
      "\u001b[92m Episode 420 finished after 300 steps\n",
      "\u001b[92m Episode 430 finished after 500 steps\n",
      "\u001b[92m Episode 440 finished after 381 steps\n",
      "\u001b[92m Episode 450 finished after 314 steps\n",
      "\u001b[92m Episode 460 finished after 247 steps\n",
      "\u001b[92m Episode 470 finished after 253 steps\n",
      "\u001b[92m Episode 480 finished after 290 steps\n",
      "\u001b[92m Episode 490 finished after 373 steps\n",
      "\u001b[92m Episode 500 finished after 375 steps\n",
      "\u001b[92m Episode 510 finished after 223 steps\n",
      "\u001b[92m Episode 520 finished after 205 steps\n",
      "\u001b[99m Episode 530 finished after 169 steps\n",
      "\u001b[99m Episode 540 finished after 118 steps\n",
      "\u001b[92m Episode 550 finished after 305 steps\n",
      "\u001b[92m Episode 560 finished after 428 steps\n",
      "\u001b[92m Episode 570 finished after 323 steps\n",
      "\u001b[92m Episode 580 finished after 247 steps\n",
      "\u001b[99m Episode 590 finished after 158 steps\n",
      "\u001b[99m Episode 600 finished after 118 steps\n",
      "\u001b[99m Episode 610 finished after 178 steps\n",
      "\u001b[92m Episode 620 finished after 269 steps\n",
      "\u001b[92m Episode 630 finished after 212 steps\n",
      "\u001b[92m Episode 640 finished after 331 steps\n",
      "\u001b[92m Episode 650 finished after 341 steps\n",
      "\u001b[92m Episode 660 finished after 315 steps\n",
      "\u001b[92m Episode 670 finished after 254 steps\n",
      "\u001b[92m Episode 680 finished after 248 steps\n",
      "\u001b[92m Episode 690 finished after 373 steps\n",
      "\u001b[92m Episode 700 finished after 324 steps\n",
      "\u001b[92m Episode 710 finished after 354 steps\n",
      "\u001b[92m Episode 720 finished after 247 steps\n",
      "\u001b[92m Episode 730 finished after 200 steps\n",
      "\u001b[92m Episode 740 finished after 339 steps\n",
      "\u001b[92m Episode 750 finished after 230 steps\n",
      "\u001b[92m Episode 760 finished after 224 steps\n",
      "\u001b[92m Episode 770 finished after 328 steps\n",
      "\u001b[92m Episode 780 finished after 251 steps\n",
      "\u001b[92m Episode 790 finished after 234 steps\n",
      "\u001b[92m Episode 800 finished after 266 steps\n",
      "\u001b[92m Episode 810 finished after 237 steps\n",
      "\u001b[92m Episode 820 finished after 258 steps\n",
      "\u001b[92m Episode 830 finished after 384 steps\n",
      "\u001b[92m Episode 840 finished after 243 steps\n",
      "\u001b[92m Episode 850 finished after 381 steps\n",
      "\u001b[92m Episode 860 finished after 273 steps\n",
      "\u001b[92m Episode 870 finished after 258 steps\n",
      "\u001b[92m Episode 880 finished after 500 steps\n",
      "\u001b[92m Episode 890 finished after 275 steps\n",
      "\u001b[99m Episode 900 finished after 152 steps\n",
      "\u001b[92m Episode 910 finished after 490 steps\n",
      "\u001b[99m Episode 920 finished after 142 steps\n",
      "\u001b[92m Episode 930 finished after 397 steps\n",
      "\u001b[99m Episode 940 finished after 185 steps\n",
      "\u001b[92m Episode 950 finished after 272 steps\n",
      "\u001b[92m Episode 960 finished after 352 steps\n",
      "\u001b[92m Episode 970 finished after 289 steps\n",
      "\u001b[92m Episode 980 finished after 381 steps\n",
      "\u001b[99m Episode 990 finished after 188 steps\n",
      "it took  542.7638351917267  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 500 steps\n",
      "\u001b[92m Episode 20 finished after 500 steps\n",
      "\u001b[92m Episode 30 finished after 357 steps\n",
      "\u001b[92m Episode 40 finished after 228 steps\n",
      "\u001b[99m Episode 50 finished after 135 steps\n",
      "\u001b[92m Episode 60 finished after 230 steps\n",
      "\u001b[92m Episode 70 finished after 227 steps\n",
      "\u001b[92m Episode 80 finished after 497 steps\n",
      "\u001b[99m Episode 90 finished after 187 steps\n",
      "\u001b[92m Episode 100 finished after 349 steps\n",
      "\u001b[99m Episode 110 finished after 151 steps\n",
      "\u001b[92m Episode 120 finished after 296 steps\n",
      "\u001b[92m Episode 130 finished after 262 steps\n",
      "\u001b[92m Episode 140 finished after 264 steps\n",
      "\u001b[92m Episode 150 finished after 334 steps\n",
      "\u001b[92m Episode 160 finished after 500 steps\n",
      "\u001b[99m Episode 170 finished after 178 steps\n",
      "\u001b[99m Episode 180 finished after 161 steps\n",
      "\u001b[92m Episode 190 finished after 500 steps\n",
      "\u001b[92m Episode 200 finished after 328 steps\n",
      "\u001b[92m Episode 210 finished after 357 steps\n",
      "\u001b[92m Episode 220 finished after 370 steps\n",
      "\u001b[92m Episode 230 finished after 500 steps\n",
      "\u001b[92m Episode 240 finished after 460 steps\n",
      "\u001b[92m Episode 250 finished after 322 steps\n",
      "\u001b[92m Episode 260 finished after 231 steps\n",
      "\u001b[92m Episode 270 finished after 261 steps\n",
      "\u001b[92m Episode 280 finished after 277 steps\n",
      "\u001b[92m Episode 290 finished after 224 steps\n",
      "\u001b[99m Episode 300 finished after 183 steps\n",
      "\u001b[92m Episode 310 finished after 451 steps\n",
      "\u001b[92m Episode 320 finished after 275 steps\n",
      "\u001b[92m Episode 330 finished after 471 steps\n",
      "\u001b[92m Episode 340 finished after 249 steps\n",
      "\u001b[92m Episode 350 finished after 340 steps\n",
      "\u001b[92m Episode 360 finished after 264 steps\n",
      "\u001b[92m Episode 370 finished after 393 steps\n",
      "\u001b[92m Episode 380 finished after 293 steps\n",
      "\u001b[92m Episode 390 finished after 245 steps\n",
      "\u001b[99m Episode 400 finished after 163 steps\n",
      "\u001b[92m Episode 410 finished after 214 steps\n",
      "\u001b[92m Episode 420 finished after 221 steps\n",
      "\u001b[92m Episode 430 finished after 271 steps\n",
      "\u001b[92m Episode 440 finished after 321 steps\n",
      "\u001b[92m Episode 450 finished after 231 steps\n",
      "\u001b[99m Episode 460 finished after 194 steps\n",
      "\u001b[92m Episode 470 finished after 348 steps\n",
      "\u001b[99m Episode 480 finished after 191 steps\n",
      "\u001b[92m Episode 490 finished after 206 steps\n",
      "\u001b[92m Episode 500 finished after 252 steps\n",
      "\u001b[92m Episode 510 finished after 288 steps\n",
      "\u001b[92m Episode 520 finished after 200 steps\n",
      "\u001b[99m Episode 530 finished after 177 steps\n",
      "\u001b[92m Episode 540 finished after 500 steps\n",
      "\u001b[92m Episode 550 finished after 500 steps\n",
      "\u001b[92m Episode 560 finished after 309 steps\n",
      "\u001b[92m Episode 570 finished after 223 steps\n",
      "\u001b[92m Episode 580 finished after 396 steps\n",
      "\u001b[92m Episode 590 finished after 276 steps\n",
      "\u001b[99m Episode 600 finished after 176 steps\n",
      "\u001b[92m Episode 610 finished after 240 steps\n",
      "\u001b[92m Episode 620 finished after 202 steps\n",
      "\u001b[92m Episode 630 finished after 396 steps\n",
      "\u001b[92m Episode 640 finished after 205 steps\n",
      "\u001b[92m Episode 650 finished after 229 steps\n",
      "\u001b[92m Episode 660 finished after 239 steps\n",
      "\u001b[92m Episode 670 finished after 229 steps\n",
      "\u001b[92m Episode 680 finished after 242 steps\n",
      "\u001b[92m Episode 690 finished after 441 steps\n",
      "\u001b[92m Episode 700 finished after 355 steps\n",
      "\u001b[99m Episode 710 finished after 184 steps\n",
      "\u001b[92m Episode 720 finished after 500 steps\n",
      "\u001b[92m Episode 730 finished after 248 steps\n",
      "\u001b[92m Episode 740 finished after 298 steps\n",
      "\u001b[92m Episode 750 finished after 431 steps\n",
      "\u001b[92m Episode 760 finished after 235 steps\n",
      "\u001b[92m Episode 770 finished after 287 steps\n",
      "\u001b[92m Episode 780 finished after 313 steps\n",
      "\u001b[92m Episode 790 finished after 247 steps\n",
      "\u001b[99m Episode 800 finished after 184 steps\n",
      "\u001b[92m Episode 810 finished after 212 steps\n",
      "\u001b[92m Episode 820 finished after 205 steps\n",
      "\u001b[92m Episode 830 finished after 207 steps\n",
      "\u001b[92m Episode 840 finished after 224 steps\n",
      "\u001b[92m Episode 850 finished after 223 steps\n",
      "\u001b[99m Episode 860 finished after 188 steps\n",
      "\u001b[92m Episode 870 finished after 198 steps\n",
      "\u001b[92m Episode 880 finished after 278 steps\n",
      "\u001b[92m Episode 890 finished after 263 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 900 finished after 233 steps\n",
      "\u001b[92m Episode 910 finished after 247 steps\n",
      "\u001b[92m Episode 920 finished after 286 steps\n",
      "\u001b[99m Episode 930 finished after 156 steps\n",
      "\u001b[92m Episode 940 finished after 255 steps\n",
      "\u001b[92m Episode 950 finished after 261 steps\n",
      "\u001b[92m Episode 960 finished after 340 steps\n",
      "\u001b[92m Episode 970 finished after 274 steps\n",
      "\u001b[99m Episode 980 finished after 176 steps\n",
      "\u001b[92m Episode 990 finished after 214 steps\n",
      "it took  593.4132721424103  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[99m Episode 10 finished after 174 steps\n",
      "\u001b[92m Episode 20 finished after 270 steps\n",
      "\u001b[92m Episode 30 finished after 287 steps\n",
      "\u001b[92m Episode 40 finished after 489 steps\n",
      "\u001b[92m Episode 50 finished after 199 steps\n",
      "\u001b[92m Episode 60 finished after 337 steps\n",
      "\u001b[92m Episode 70 finished after 245 steps\n",
      "\u001b[99m Episode 80 finished after 182 steps\n",
      "\u001b[92m Episode 90 finished after 349 steps\n",
      "\u001b[92m Episode 100 finished after 307 steps\n",
      "\u001b[99m Episode 110 finished after 174 steps\n",
      "\u001b[92m Episode 120 finished after 329 steps\n",
      "\u001b[92m Episode 130 finished after 378 steps\n",
      "\u001b[92m Episode 140 finished after 357 steps\n",
      "\u001b[92m Episode 150 finished after 246 steps\n",
      "\u001b[92m Episode 160 finished after 226 steps\n",
      "\u001b[92m Episode 170 finished after 346 steps\n",
      "\u001b[92m Episode 180 finished after 258 steps\n",
      "\u001b[92m Episode 190 finished after 235 steps\n",
      "\u001b[92m Episode 200 finished after 238 steps\n",
      "\u001b[92m Episode 210 finished after 299 steps\n",
      "\u001b[92m Episode 220 finished after 247 steps\n",
      "\u001b[92m Episode 230 finished after 255 steps\n",
      "\u001b[92m Episode 240 finished after 307 steps\n",
      "\u001b[92m Episode 250 finished after 500 steps\n",
      "\u001b[92m Episode 260 finished after 372 steps\n",
      "\u001b[92m Episode 270 finished after 337 steps\n",
      "\u001b[92m Episode 280 finished after 221 steps\n",
      "\u001b[92m Episode 290 finished after 298 steps\n",
      "\u001b[99m Episode 300 finished after 188 steps\n",
      "\u001b[92m Episode 310 finished after 328 steps\n",
      "\u001b[92m Episode 320 finished after 207 steps\n",
      "\u001b[92m Episode 330 finished after 366 steps\n",
      "\u001b[92m Episode 340 finished after 299 steps\n",
      "\u001b[92m Episode 350 finished after 317 steps\n",
      "\u001b[92m Episode 360 finished after 477 steps\n",
      "\u001b[92m Episode 370 finished after 347 steps\n",
      "\u001b[92m Episode 380 finished after 224 steps\n",
      "\u001b[92m Episode 390 finished after 419 steps\n",
      "\u001b[92m Episode 400 finished after 404 steps\n",
      "\u001b[92m Episode 410 finished after 392 steps\n",
      "\u001b[92m Episode 420 finished after 234 steps\n",
      "\u001b[92m Episode 430 finished after 370 steps\n",
      "\u001b[99m Episode 440 finished after 188 steps\n",
      "\u001b[92m Episode 450 finished after 276 steps\n",
      "\u001b[99m Episode 460 finished after 139 steps\n",
      "\u001b[99m Episode 470 finished after 181 steps\n",
      "\u001b[92m Episode 480 finished after 391 steps\n",
      "\u001b[92m Episode 490 finished after 330 steps\n",
      "\u001b[92m Episode 500 finished after 500 steps\n",
      "\u001b[99m Episode 510 finished after 148 steps\n",
      "\u001b[99m Episode 520 finished after 162 steps\n",
      "\u001b[92m Episode 530 finished after 441 steps\n",
      "\u001b[92m Episode 540 finished after 500 steps\n",
      "\u001b[92m Episode 550 finished after 260 steps\n",
      "\u001b[92m Episode 560 finished after 298 steps\n",
      "\u001b[92m Episode 570 finished after 233 steps\n",
      "\u001b[92m Episode 580 finished after 492 steps\n",
      "\u001b[92m Episode 590 finished after 396 steps\n",
      "\u001b[92m Episode 600 finished after 381 steps\n",
      "\u001b[92m Episode 610 finished after 292 steps\n",
      "\u001b[92m Episode 620 finished after 250 steps\n",
      "\u001b[92m Episode 630 finished after 500 steps\n",
      "\u001b[92m Episode 640 finished after 218 steps\n",
      "\u001b[92m Episode 650 finished after 252 steps\n",
      "\u001b[92m Episode 660 finished after 269 steps\n",
      "\u001b[92m Episode 670 finished after 500 steps\n",
      "\u001b[92m Episode 680 finished after 419 steps\n",
      "\u001b[92m Episode 690 finished after 285 steps\n",
      "\u001b[92m Episode 700 finished after 290 steps\n",
      "\u001b[92m Episode 710 finished after 301 steps\n",
      "\u001b[92m Episode 720 finished after 366 steps\n",
      "\u001b[92m Episode 730 finished after 313 steps\n",
      "\u001b[92m Episode 740 finished after 229 steps\n",
      "\u001b[92m Episode 750 finished after 278 steps\n",
      "\u001b[92m Episode 760 finished after 322 steps\n",
      "\u001b[92m Episode 770 finished after 317 steps\n",
      "\u001b[92m Episode 780 finished after 348 steps\n",
      "\u001b[99m Episode 790 finished after 151 steps\n",
      "\u001b[92m Episode 800 finished after 195 steps\n",
      "\u001b[92m Episode 810 finished after 207 steps\n",
      "\u001b[99m Episode 820 finished after 148 steps\n",
      "\u001b[99m Episode 830 finished after 183 steps\n",
      "\u001b[92m Episode 840 finished after 254 steps\n",
      "\u001b[92m Episode 850 finished after 340 steps\n",
      "\u001b[92m Episode 860 finished after 222 steps\n",
      "\u001b[92m Episode 870 finished after 382 steps\n",
      "\u001b[92m Episode 880 finished after 313 steps\n",
      "\u001b[92m Episode 890 finished after 337 steps\n",
      "\u001b[92m Episode 900 finished after 201 steps\n",
      "\u001b[92m Episode 910 finished after 387 steps\n",
      "\u001b[99m Episode 920 finished after 181 steps\n",
      "\u001b[92m Episode 930 finished after 259 steps\n",
      "\u001b[92m Episode 940 finished after 264 steps\n",
      "\u001b[92m Episode 950 finished after 215 steps\n",
      "\u001b[92m Episode 960 finished after 255 steps\n",
      "\u001b[92m Episode 970 finished after 500 steps\n",
      "\u001b[99m Episode 980 finished after 135 steps\n",
      "\u001b[92m Episode 990 finished after 342 steps\n",
      "it took  515.527440071106  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 216 steps\n",
      "\u001b[92m Episode 20 finished after 281 steps\n",
      "\u001b[92m Episode 30 finished after 500 steps\n",
      "\u001b[92m Episode 40 finished after 451 steps\n",
      "\u001b[92m Episode 50 finished after 500 steps\n",
      "\u001b[99m Episode 60 finished after 148 steps\n",
      "\u001b[92m Episode 70 finished after 201 steps\n",
      "\u001b[92m Episode 80 finished after 500 steps\n",
      "\u001b[92m Episode 90 finished after 323 steps\n",
      "\u001b[92m Episode 100 finished after 390 steps\n",
      "\u001b[92m Episode 110 finished after 238 steps\n",
      "\u001b[92m Episode 120 finished after 244 steps\n",
      "\u001b[92m Episode 130 finished after 371 steps\n",
      "\u001b[92m Episode 140 finished after 202 steps\n",
      "\u001b[92m Episode 150 finished after 316 steps\n",
      "\u001b[92m Episode 160 finished after 290 steps\n",
      "\u001b[92m Episode 170 finished after 396 steps\n",
      "\u001b[92m Episode 180 finished after 232 steps\n",
      "\u001b[92m Episode 190 finished after 207 steps\n",
      "\u001b[92m Episode 200 finished after 264 steps\n",
      "\u001b[92m Episode 210 finished after 318 steps\n",
      "\u001b[92m Episode 220 finished after 268 steps\n",
      "\u001b[99m Episode 230 finished after 181 steps\n",
      "\u001b[99m Episode 240 finished after 178 steps\n",
      "\u001b[92m Episode 250 finished after 276 steps\n",
      "\u001b[92m Episode 260 finished after 487 steps\n",
      "\u001b[92m Episode 270 finished after 428 steps\n",
      "\u001b[99m Episode 280 finished after 190 steps\n",
      "\u001b[99m Episode 290 finished after 186 steps\n",
      "\u001b[92m Episode 300 finished after 272 steps\n",
      "\u001b[92m Episode 310 finished after 286 steps\n",
      "\u001b[92m Episode 320 finished after 270 steps\n",
      "\u001b[92m Episode 330 finished after 418 steps\n",
      "\u001b[92m Episode 340 finished after 389 steps\n",
      "\u001b[92m Episode 350 finished after 255 steps\n",
      "\u001b[99m Episode 360 finished after 146 steps\n",
      "\u001b[92m Episode 370 finished after 196 steps\n",
      "\u001b[92m Episode 380 finished after 267 steps\n",
      "\u001b[92m Episode 390 finished after 344 steps\n",
      "\u001b[92m Episode 400 finished after 414 steps\n",
      "\u001b[92m Episode 410 finished after 264 steps\n",
      "\u001b[92m Episode 420 finished after 281 steps\n",
      "\u001b[99m Episode 430 finished after 171 steps\n",
      "\u001b[92m Episode 440 finished after 247 steps\n",
      "\u001b[92m Episode 450 finished after 198 steps\n",
      "\u001b[92m Episode 460 finished after 219 steps\n",
      "\u001b[92m Episode 470 finished after 210 steps\n",
      "\u001b[92m Episode 480 finished after 195 steps\n",
      "\u001b[92m Episode 490 finished after 217 steps\n",
      "\u001b[92m Episode 500 finished after 240 steps\n",
      "\u001b[99m Episode 510 finished after 170 steps\n",
      "\u001b[99m Episode 520 finished after 166 steps\n",
      "\u001b[92m Episode 530 finished after 208 steps\n",
      "\u001b[99m Episode 540 finished after 187 steps\n",
      "\u001b[99m Episode 550 finished after 163 steps\n",
      "\u001b[92m Episode 560 finished after 284 steps\n",
      "\u001b[92m Episode 570 finished after 260 steps\n",
      "\u001b[92m Episode 580 finished after 239 steps\n",
      "\u001b[92m Episode 590 finished after 253 steps\n",
      "\u001b[92m Episode 600 finished after 280 steps\n",
      "\u001b[92m Episode 610 finished after 204 steps\n",
      "\u001b[92m Episode 620 finished after 327 steps\n",
      "\u001b[92m Episode 630 finished after 302 steps\n",
      "\u001b[92m Episode 640 finished after 284 steps\n",
      "\u001b[92m Episode 650 finished after 274 steps\n",
      "\u001b[99m Episode 660 finished after 194 steps\n",
      "\u001b[92m Episode 670 finished after 228 steps\n",
      "\u001b[92m Episode 680 finished after 344 steps\n",
      "\u001b[92m Episode 690 finished after 244 steps\n",
      "\u001b[92m Episode 700 finished after 271 steps\n",
      "\u001b[92m Episode 710 finished after 396 steps\n",
      "\u001b[92m Episode 720 finished after 306 steps\n",
      "\u001b[92m Episode 730 finished after 269 steps\n",
      "\u001b[99m Episode 740 finished after 186 steps\n",
      "\u001b[92m Episode 750 finished after 197 steps\n",
      "\u001b[92m Episode 760 finished after 212 steps\n",
      "\u001b[92m Episode 770 finished after 200 steps\n",
      "\u001b[92m Episode 780 finished after 240 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 790 finished after 103 steps\n",
      "\u001b[92m Episode 800 finished after 207 steps\n",
      "\u001b[99m Episode 810 finished after 159 steps\n",
      "\u001b[92m Episode 820 finished after 434 steps\n",
      "\u001b[92m Episode 830 finished after 302 steps\n",
      "\u001b[99m Episode 840 finished after 100 steps\n",
      "\u001b[99m Episode 850 finished after 142 steps\n",
      "\u001b[92m Episode 860 finished after 311 steps\n",
      "\u001b[92m Episode 870 finished after 500 steps\n",
      "\u001b[92m Episode 880 finished after 347 steps\n",
      "\u001b[92m Episode 890 finished after 258 steps\n",
      "\u001b[99m Episode 900 finished after 144 steps\n",
      "\u001b[92m Episode 910 finished after 289 steps\n",
      "\u001b[92m Episode 920 finished after 211 steps\n",
      "\u001b[92m Episode 930 finished after 326 steps\n",
      "\u001b[92m Episode 940 finished after 240 steps\n",
      "\u001b[92m Episode 950 finished after 340 steps\n",
      "\u001b[92m Episode 960 finished after 374 steps\n",
      "\u001b[92m Episode 970 finished after 301 steps\n",
      "\u001b[92m Episode 980 finished after 407 steps\n",
      "\u001b[99m Episode 990 finished after 173 steps\n",
      "it took  526.6960489749908  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 500 steps\n",
      "\u001b[92m Episode 20 finished after 385 steps\n",
      "\u001b[92m Episode 30 finished after 500 steps\n",
      "\u001b[92m Episode 40 finished after 500 steps\n",
      "\u001b[99m Episode 50 finished after 175 steps\n",
      "\u001b[92m Episode 60 finished after 306 steps\n",
      "\u001b[92m Episode 70 finished after 250 steps\n",
      "\u001b[92m Episode 80 finished after 399 steps\n",
      "\u001b[92m Episode 90 finished after 267 steps\n",
      "\u001b[92m Episode 100 finished after 401 steps\n",
      "\u001b[92m Episode 110 finished after 214 steps\n",
      "\u001b[92m Episode 120 finished after 378 steps\n",
      "\u001b[92m Episode 130 finished after 500 steps\n",
      "\u001b[92m Episode 140 finished after 266 steps\n",
      "\u001b[92m Episode 150 finished after 342 steps\n",
      "\u001b[99m Episode 160 finished after 166 steps\n",
      "\u001b[92m Episode 170 finished after 271 steps\n",
      "\u001b[92m Episode 180 finished after 326 steps\n",
      "\u001b[92m Episode 190 finished after 440 steps\n",
      "\u001b[92m Episode 200 finished after 262 steps\n",
      "\u001b[92m Episode 210 finished after 288 steps\n",
      "\u001b[92m Episode 220 finished after 367 steps\n",
      "\u001b[92m Episode 230 finished after 328 steps\n",
      "\u001b[92m Episode 240 finished after 218 steps\n",
      "\u001b[92m Episode 250 finished after 249 steps\n",
      "\u001b[92m Episode 260 finished after 360 steps\n",
      "\u001b[92m Episode 270 finished after 226 steps\n",
      "\u001b[92m Episode 280 finished after 221 steps\n",
      "\u001b[92m Episode 290 finished after 280 steps\n",
      "\u001b[92m Episode 300 finished after 269 steps\n",
      "\u001b[92m Episode 310 finished after 265 steps\n",
      "\u001b[92m Episode 320 finished after 339 steps\n",
      "\u001b[92m Episode 330 finished after 397 steps\n",
      "\u001b[92m Episode 340 finished after 412 steps\n",
      "\u001b[92m Episode 350 finished after 365 steps\n",
      "\u001b[99m Episode 360 finished after 172 steps\n",
      "\u001b[92m Episode 370 finished after 322 steps\n",
      "\u001b[92m Episode 380 finished after 274 steps\n",
      "\u001b[92m Episode 390 finished after 253 steps\n",
      "\u001b[92m Episode 400 finished after 375 steps\n",
      "\u001b[92m Episode 410 finished after 348 steps\n",
      "\u001b[92m Episode 420 finished after 241 steps\n",
      "\u001b[92m Episode 430 finished after 206 steps\n",
      "\u001b[92m Episode 440 finished after 235 steps\n",
      "\u001b[92m Episode 450 finished after 352 steps\n",
      "\u001b[99m Episode 460 finished after 146 steps\n",
      "\u001b[92m Episode 470 finished after 292 steps\n",
      "\u001b[92m Episode 480 finished after 450 steps\n",
      "\u001b[92m Episode 490 finished after 285 steps\n",
      "\u001b[99m Episode 500 finished after 102 steps\n",
      "\u001b[92m Episode 510 finished after 251 steps\n",
      "\u001b[92m Episode 520 finished after 207 steps\n",
      "\u001b[99m Episode 530 finished after 160 steps\n",
      "\u001b[92m Episode 540 finished after 196 steps\n",
      "\u001b[99m Episode 550 finished after 180 steps\n",
      "\u001b[92m Episode 560 finished after 289 steps\n",
      "\u001b[99m Episode 570 finished after 120 steps\n",
      "\u001b[99m Episode 580 finished after 168 steps\n",
      "\u001b[92m Episode 590 finished after 252 steps\n",
      "\u001b[92m Episode 600 finished after 209 steps\n",
      "\u001b[92m Episode 610 finished after 314 steps\n",
      "\u001b[92m Episode 620 finished after 203 steps\n",
      "\u001b[92m Episode 630 finished after 322 steps\n",
      "\u001b[99m Episode 640 finished after 188 steps\n",
      "\u001b[92m Episode 650 finished after 300 steps\n",
      "\u001b[92m Episode 660 finished after 306 steps\n",
      "\u001b[92m Episode 670 finished after 206 steps\n",
      "\u001b[92m Episode 680 finished after 262 steps\n",
      "\u001b[99m Episode 690 finished after 186 steps\n",
      "\u001b[92m Episode 700 finished after 260 steps\n",
      "\u001b[92m Episode 710 finished after 205 steps\n",
      "\u001b[92m Episode 720 finished after 247 steps\n",
      "\u001b[92m Episode 730 finished after 292 steps\n",
      "\u001b[92m Episode 740 finished after 397 steps\n",
      "\u001b[99m Episode 750 finished after 174 steps\n",
      "\u001b[92m Episode 760 finished after 212 steps\n",
      "\u001b[92m Episode 770 finished after 229 steps\n",
      "\u001b[92m Episode 780 finished after 241 steps\n",
      "\u001b[99m Episode 790 finished after 185 steps\n",
      "\u001b[92m Episode 800 finished after 275 steps\n",
      "\u001b[92m Episode 810 finished after 279 steps\n",
      "\u001b[92m Episode 820 finished after 280 steps\n",
      "\u001b[99m Episode 830 finished after 179 steps\n",
      "\u001b[92m Episode 840 finished after 216 steps\n",
      "\u001b[92m Episode 850 finished after 206 steps\n",
      "\u001b[92m Episode 860 finished after 212 steps\n",
      "\u001b[99m Episode 870 finished after 171 steps\n",
      "\u001b[92m Episode 880 finished after 296 steps\n",
      "\u001b[99m Episode 890 finished after 149 steps\n",
      "\u001b[92m Episode 900 finished after 195 steps\n",
      "\u001b[99m Episode 910 finished after 154 steps\n",
      "\u001b[99m Episode 920 finished after 134 steps\n",
      "\u001b[99m Episode 930 finished after 119 steps\n",
      "\u001b[99m Episode 940 finished after 165 steps\n",
      "\u001b[92m Episode 950 finished after 244 steps\n",
      "\u001b[92m Episode 960 finished after 246 steps\n",
      "\u001b[92m Episode 970 finished after 265 steps\n",
      "\u001b[92m Episode 980 finished after 259 steps\n",
      "\u001b[92m Episode 990 finished after 249 steps\n",
      "it took  664.7678050994873  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 286 steps\n",
      "\u001b[92m Episode 20 finished after 222 steps\n",
      "\u001b[92m Episode 30 finished after 237 steps\n",
      "\u001b[92m Episode 40 finished after 393 steps\n",
      "\u001b[92m Episode 50 finished after 295 steps\n",
      "\u001b[92m Episode 60 finished after 214 steps\n",
      "\u001b[92m Episode 70 finished after 387 steps\n",
      "\u001b[92m Episode 80 finished after 284 steps\n",
      "\u001b[92m Episode 90 finished after 379 steps\n",
      "\u001b[92m Episode 100 finished after 300 steps\n",
      "\u001b[92m Episode 110 finished after 222 steps\n",
      "\u001b[99m Episode 120 finished after 157 steps\n",
      "\u001b[92m Episode 130 finished after 287 steps\n",
      "\u001b[92m Episode 140 finished after 410 steps\n",
      "\u001b[92m Episode 150 finished after 303 steps\n",
      "\u001b[92m Episode 160 finished after 266 steps\n",
      "\u001b[92m Episode 170 finished after 272 steps\n",
      "\u001b[92m Episode 180 finished after 401 steps\n",
      "\u001b[92m Episode 190 finished after 251 steps\n",
      "\u001b[92m Episode 200 finished after 381 steps\n",
      "\u001b[92m Episode 210 finished after 348 steps\n",
      "\u001b[99m Episode 220 finished after 184 steps\n",
      "\u001b[92m Episode 230 finished after 296 steps\n",
      "\u001b[92m Episode 240 finished after 500 steps\n",
      "\u001b[92m Episode 250 finished after 361 steps\n",
      "\u001b[92m Episode 260 finished after 253 steps\n",
      "\u001b[99m Episode 270 finished after 193 steps\n",
      "\u001b[92m Episode 280 finished after 278 steps\n",
      "\u001b[92m Episode 290 finished after 291 steps\n",
      "\u001b[99m Episode 300 finished after 188 steps\n",
      "\u001b[92m Episode 310 finished after 286 steps\n",
      "\u001b[92m Episode 320 finished after 392 steps\n",
      "\u001b[92m Episode 330 finished after 195 steps\n",
      "\u001b[92m Episode 340 finished after 232 steps\n",
      "\u001b[99m Episode 350 finished after 192 steps\n",
      "\u001b[92m Episode 360 finished after 385 steps\n",
      "\u001b[92m Episode 370 finished after 500 steps\n",
      "\u001b[92m Episode 380 finished after 270 steps\n",
      "\u001b[92m Episode 390 finished after 239 steps\n",
      "\u001b[92m Episode 400 finished after 471 steps\n",
      "\u001b[92m Episode 410 finished after 251 steps\n",
      "\u001b[92m Episode 420 finished after 265 steps\n",
      "\u001b[99m Episode 430 finished after 191 steps\n",
      "\u001b[99m Episode 440 finished after 193 steps\n",
      "\u001b[92m Episode 450 finished after 199 steps\n",
      "\u001b[99m Episode 460 finished after 186 steps\n",
      "\u001b[92m Episode 470 finished after 345 steps\n",
      "\u001b[99m Episode 480 finished after 132 steps\n",
      "\u001b[92m Episode 490 finished after 257 steps\n",
      "\u001b[92m Episode 500 finished after 225 steps\n",
      "\u001b[92m Episode 510 finished after 231 steps\n",
      "\u001b[99m Episode 520 finished after 141 steps\n",
      "\u001b[99m Episode 530 finished after 179 steps\n",
      "\u001b[99m Episode 540 finished after 174 steps\n",
      "\u001b[99m Episode 550 finished after 178 steps\n",
      "\u001b[99m Episode 560 finished after 142 steps\n",
      "\u001b[92m Episode 570 finished after 228 steps\n",
      "\u001b[92m Episode 580 finished after 255 steps\n",
      "\u001b[92m Episode 590 finished after 317 steps\n",
      "\u001b[92m Episode 600 finished after 201 steps\n",
      "\u001b[99m Episode 610 finished after 193 steps\n",
      "\u001b[92m Episode 620 finished after 296 steps\n",
      "\u001b[92m Episode 630 finished after 341 steps\n",
      "\u001b[92m Episode 640 finished after 212 steps\n",
      "\u001b[92m Episode 650 finished after 274 steps\n",
      "\u001b[92m Episode 660 finished after 315 steps\n",
      "\u001b[92m Episode 670 finished after 232 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 680 finished after 172 steps\n",
      "\u001b[92m Episode 690 finished after 216 steps\n",
      "\u001b[92m Episode 700 finished after 245 steps\n",
      "\u001b[92m Episode 710 finished after 392 steps\n",
      "\u001b[92m Episode 720 finished after 381 steps\n",
      "\u001b[92m Episode 730 finished after 217 steps\n",
      "\u001b[92m Episode 740 finished after 338 steps\n",
      "\u001b[92m Episode 750 finished after 434 steps\n",
      "\u001b[92m Episode 760 finished after 269 steps\n",
      "\u001b[92m Episode 770 finished after 365 steps\n",
      "\u001b[92m Episode 780 finished after 404 steps\n",
      "\u001b[92m Episode 790 finished after 260 steps\n",
      "\u001b[99m Episode 800 finished after 190 steps\n",
      "\u001b[99m Episode 810 finished after 138 steps\n",
      "\u001b[99m Episode 820 finished after 150 steps\n",
      "\u001b[92m Episode 830 finished after 256 steps\n",
      "\u001b[99m Episode 840 finished after 179 steps\n",
      "\u001b[92m Episode 850 finished after 276 steps\n",
      "\u001b[92m Episode 860 finished after 500 steps\n",
      "\u001b[92m Episode 870 finished after 200 steps\n",
      "\u001b[92m Episode 880 finished after 205 steps\n",
      "\u001b[92m Episode 890 finished after 305 steps\n",
      "\u001b[92m Episode 900 finished after 323 steps\n",
      "\u001b[92m Episode 910 finished after 468 steps\n",
      "\u001b[92m Episode 920 finished after 330 steps\n",
      "\u001b[92m Episode 930 finished after 274 steps\n",
      "\u001b[92m Episode 940 finished after 254 steps\n",
      "\u001b[92m Episode 950 finished after 348 steps\n",
      "\u001b[92m Episode 960 finished after 222 steps\n",
      "\u001b[99m Episode 970 finished after 118 steps\n",
      "\u001b[92m Episode 980 finished after 228 steps\n",
      "\u001b[92m Episode 990 finished after 247 steps\n",
      "it took  547.3497469425201  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 249 steps\n",
      "\u001b[92m Episode 20 finished after 301 steps\n",
      "\u001b[92m Episode 30 finished after 227 steps\n",
      "\u001b[92m Episode 40 finished after 388 steps\n",
      "\u001b[92m Episode 50 finished after 254 steps\n",
      "\u001b[92m Episode 60 finished after 201 steps\n",
      "\u001b[92m Episode 70 finished after 216 steps\n",
      "\u001b[92m Episode 80 finished after 378 steps\n",
      "\u001b[92m Episode 90 finished after 312 steps\n",
      "\u001b[99m Episode 100 finished after 176 steps\n",
      "\u001b[92m Episode 110 finished after 291 steps\n",
      "\u001b[92m Episode 120 finished after 431 steps\n",
      "\u001b[92m Episode 130 finished after 382 steps\n",
      "\u001b[92m Episode 140 finished after 207 steps\n",
      "\u001b[92m Episode 150 finished after 354 steps\n",
      "\u001b[92m Episode 160 finished after 334 steps\n",
      "\u001b[92m Episode 170 finished after 230 steps\n",
      "\u001b[92m Episode 180 finished after 491 steps\n",
      "\u001b[92m Episode 190 finished after 290 steps\n",
      "\u001b[99m Episode 200 finished after 174 steps\n",
      "\u001b[92m Episode 210 finished after 330 steps\n",
      "\u001b[92m Episode 220 finished after 396 steps\n",
      "\u001b[92m Episode 230 finished after 200 steps\n",
      "\u001b[92m Episode 240 finished after 331 steps\n",
      "\u001b[92m Episode 250 finished after 411 steps\n",
      "\u001b[92m Episode 260 finished after 300 steps\n",
      "\u001b[92m Episode 270 finished after 323 steps\n",
      "\u001b[92m Episode 280 finished after 500 steps\n",
      "\u001b[92m Episode 290 finished after 350 steps\n",
      "\u001b[92m Episode 300 finished after 326 steps\n",
      "\u001b[92m Episode 310 finished after 367 steps\n",
      "\u001b[92m Episode 320 finished after 374 steps\n",
      "\u001b[92m Episode 330 finished after 286 steps\n",
      "\u001b[92m Episode 340 finished after 253 steps\n",
      "\u001b[99m Episode 350 finished after 151 steps\n",
      "\u001b[99m Episode 360 finished after 189 steps\n",
      "\u001b[92m Episode 370 finished after 282 steps\n",
      "\u001b[92m Episode 380 finished after 288 steps\n",
      "\u001b[92m Episode 390 finished after 412 steps\n",
      "\u001b[92m Episode 400 finished after 336 steps\n",
      "\u001b[92m Episode 410 finished after 271 steps\n",
      "\u001b[92m Episode 420 finished after 453 steps\n",
      "\u001b[92m Episode 430 finished after 455 steps\n",
      "\u001b[99m Episode 440 finished after 185 steps\n",
      "\u001b[92m Episode 450 finished after 346 steps\n",
      "\u001b[99m Episode 460 finished after 190 steps\n",
      "\u001b[92m Episode 470 finished after 413 steps\n",
      "\u001b[92m Episode 480 finished after 248 steps\n",
      "\u001b[92m Episode 490 finished after 400 steps\n",
      "\u001b[92m Episode 500 finished after 200 steps\n",
      "\u001b[92m Episode 510 finished after 343 steps\n",
      "\u001b[92m Episode 520 finished after 266 steps\n",
      "\u001b[92m Episode 530 finished after 292 steps\n",
      "\u001b[92m Episode 540 finished after 385 steps\n",
      "\u001b[99m Episode 550 finished after 190 steps\n",
      "\u001b[92m Episode 560 finished after 208 steps\n",
      "\u001b[92m Episode 570 finished after 416 steps\n",
      "\u001b[92m Episode 580 finished after 221 steps\n",
      "\u001b[92m Episode 590 finished after 310 steps\n",
      "\u001b[92m Episode 600 finished after 211 steps\n",
      "\u001b[92m Episode 610 finished after 336 steps\n",
      "\u001b[92m Episode 620 finished after 204 steps\n",
      "\u001b[92m Episode 630 finished after 244 steps\n",
      "\u001b[92m Episode 640 finished after 282 steps\n",
      "\u001b[92m Episode 650 finished after 218 steps\n",
      "\u001b[99m Episode 660 finished after 181 steps\n",
      "\u001b[92m Episode 670 finished after 229 steps\n",
      "\u001b[99m Episode 680 finished after 137 steps\n",
      "\u001b[92m Episode 690 finished after 265 steps\n",
      "\u001b[92m Episode 700 finished after 382 steps\n",
      "\u001b[92m Episode 710 finished after 278 steps\n",
      "\u001b[92m Episode 720 finished after 259 steps\n",
      "\u001b[92m Episode 730 finished after 346 steps\n",
      "\u001b[92m Episode 740 finished after 500 steps\n",
      "\u001b[99m Episode 750 finished after 158 steps\n",
      "\u001b[99m Episode 760 finished after 192 steps\n",
      "\u001b[92m Episode 770 finished after 327 steps\n",
      "\u001b[92m Episode 780 finished after 208 steps\n",
      "\u001b[92m Episode 790 finished after 409 steps\n",
      "\u001b[99m Episode 800 finished after 160 steps\n",
      "\u001b[92m Episode 810 finished after 206 steps\n",
      "\u001b[92m Episode 820 finished after 207 steps\n",
      "\u001b[92m Episode 830 finished after 220 steps\n",
      "\u001b[92m Episode 840 finished after 238 steps\n",
      "\u001b[92m Episode 850 finished after 367 steps\n",
      "\u001b[92m Episode 860 finished after 281 steps\n",
      "\u001b[99m Episode 870 finished after 186 steps\n",
      "\u001b[99m Episode 880 finished after 164 steps\n",
      "\u001b[92m Episode 890 finished after 276 steps\n",
      "\u001b[92m Episode 900 finished after 227 steps\n",
      "\u001b[92m Episode 910 finished after 278 steps\n",
      "\u001b[92m Episode 920 finished after 290 steps\n",
      "\u001b[92m Episode 930 finished after 430 steps\n",
      "\u001b[92m Episode 940 finished after 204 steps\n",
      "\u001b[92m Episode 950 finished after 287 steps\n",
      "\u001b[99m Episode 960 finished after 184 steps\n",
      "\u001b[92m Episode 970 finished after 424 steps\n",
      "\u001b[92m Episode 980 finished after 263 steps\n",
      "\u001b[92m Episode 990 finished after 420 steps\n",
      "it took  636.1336686611176  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 249 steps\n",
      "\u001b[92m Episode 20 finished after 500 steps\n",
      "\u001b[92m Episode 30 finished after 323 steps\n",
      "\u001b[92m Episode 40 finished after 283 steps\n",
      "\u001b[99m Episode 50 finished after 148 steps\n",
      "\u001b[92m Episode 60 finished after 404 steps\n",
      "\u001b[92m Episode 70 finished after 307 steps\n",
      "\u001b[99m Episode 80 finished after 188 steps\n",
      "\u001b[92m Episode 90 finished after 409 steps\n",
      "\u001b[92m Episode 100 finished after 239 steps\n",
      "\u001b[92m Episode 110 finished after 500 steps\n",
      "\u001b[92m Episode 120 finished after 222 steps\n",
      "\u001b[92m Episode 130 finished after 359 steps\n",
      "\u001b[99m Episode 140 finished after 162 steps\n",
      "\u001b[92m Episode 150 finished after 234 steps\n",
      "\u001b[92m Episode 160 finished after 412 steps\n",
      "\u001b[92m Episode 170 finished after 246 steps\n",
      "\u001b[92m Episode 180 finished after 392 steps\n",
      "\u001b[99m Episode 190 finished after 190 steps\n",
      "\u001b[92m Episode 200 finished after 230 steps\n",
      "\u001b[92m Episode 210 finished after 308 steps\n",
      "\u001b[92m Episode 220 finished after 279 steps\n",
      "\u001b[92m Episode 230 finished after 500 steps\n",
      "\u001b[92m Episode 240 finished after 209 steps\n",
      "\u001b[92m Episode 250 finished after 490 steps\n",
      "\u001b[92m Episode 260 finished after 235 steps\n",
      "\u001b[92m Episode 270 finished after 304 steps\n",
      "\u001b[92m Episode 280 finished after 207 steps\n",
      "\u001b[92m Episode 290 finished after 457 steps\n",
      "\u001b[92m Episode 300 finished after 392 steps\n",
      "\u001b[92m Episode 310 finished after 339 steps\n",
      "\u001b[99m Episode 320 finished after 155 steps\n",
      "\u001b[99m Episode 330 finished after 148 steps\n",
      "\u001b[99m Episode 340 finished after 152 steps\n",
      "\u001b[99m Episode 350 finished after 191 steps\n",
      "\u001b[92m Episode 360 finished after 290 steps\n",
      "\u001b[92m Episode 370 finished after 242 steps\n",
      "\u001b[92m Episode 380 finished after 404 steps\n",
      "\u001b[92m Episode 390 finished after 289 steps\n",
      "\u001b[92m Episode 400 finished after 326 steps\n",
      "\u001b[92m Episode 410 finished after 235 steps\n",
      "\u001b[99m Episode 420 finished after 185 steps\n",
      "\u001b[92m Episode 430 finished after 311 steps\n",
      "\u001b[92m Episode 440 finished after 258 steps\n",
      "\u001b[92m Episode 450 finished after 421 steps\n",
      "\u001b[92m Episode 460 finished after 203 steps\n",
      "\u001b[99m Episode 470 finished after 184 steps\n",
      "\u001b[92m Episode 480 finished after 294 steps\n",
      "\u001b[92m Episode 490 finished after 247 steps\n",
      "\u001b[99m Episode 500 finished after 184 steps\n",
      "\u001b[99m Episode 510 finished after 154 steps\n",
      "\u001b[99m Episode 520 finished after 134 steps\n",
      "\u001b[99m Episode 530 finished after 169 steps\n",
      "\u001b[99m Episode 540 finished after 132 steps\n",
      "\u001b[92m Episode 550 finished after 385 steps\n",
      "\u001b[92m Episode 560 finished after 346 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 570 finished after 182 steps\n",
      "\u001b[99m Episode 580 finished after 173 steps\n",
      "\u001b[92m Episode 590 finished after 384 steps\n",
      "\u001b[92m Episode 600 finished after 466 steps\n",
      "\u001b[92m Episode 610 finished after 492 steps\n",
      "\u001b[92m Episode 620 finished after 500 steps\n",
      "\u001b[92m Episode 630 finished after 247 steps\n",
      "\u001b[99m Episode 640 finished after 174 steps\n",
      "\u001b[92m Episode 650 finished after 213 steps\n",
      "\u001b[92m Episode 660 finished after 333 steps\n",
      "\u001b[92m Episode 670 finished after 200 steps\n",
      "\u001b[99m Episode 680 finished after 165 steps\n",
      "\u001b[99m Episode 690 finished after 148 steps\n",
      "\u001b[99m Episode 700 finished after 149 steps\n",
      "\u001b[92m Episode 710 finished after 380 steps\n",
      "\u001b[92m Episode 720 finished after 353 steps\n",
      "\u001b[92m Episode 730 finished after 228 steps\n",
      "\u001b[92m Episode 740 finished after 344 steps\n",
      "\u001b[92m Episode 750 finished after 500 steps\n",
      "\u001b[92m Episode 760 finished after 292 steps\n",
      "\u001b[92m Episode 770 finished after 240 steps\n",
      "\u001b[92m Episode 780 finished after 336 steps\n",
      "\u001b[92m Episode 790 finished after 327 steps\n",
      "\u001b[92m Episode 800 finished after 500 steps\n",
      "\u001b[92m Episode 810 finished after 261 steps\n",
      "\u001b[92m Episode 820 finished after 379 steps\n",
      "\u001b[92m Episode 830 finished after 202 steps\n",
      "\u001b[99m Episode 840 finished after 169 steps\n",
      "\u001b[92m Episode 850 finished after 279 steps\n",
      "\u001b[92m Episode 860 finished after 344 steps\n",
      "\u001b[92m Episode 870 finished after 379 steps\n",
      "\u001b[92m Episode 880 finished after 202 steps\n",
      "\u001b[92m Episode 890 finished after 207 steps\n",
      "\u001b[99m Episode 900 finished after 194 steps\n",
      "\u001b[92m Episode 910 finished after 320 steps\n",
      "\u001b[92m Episode 920 finished after 247 steps\n",
      "\u001b[99m Episode 930 finished after 166 steps\n",
      "\u001b[92m Episode 940 finished after 392 steps\n",
      "\u001b[92m Episode 950 finished after 328 steps\n",
      "\u001b[92m Episode 960 finished after 488 steps\n",
      "\u001b[99m Episode 970 finished after 181 steps\n",
      "\u001b[92m Episode 980 finished after 205 steps\n",
      "\u001b[92m Episode 990 finished after 257 steps\n",
      "it took  752.8117399215698  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 204 steps\n",
      "\u001b[92m Episode 20 finished after 231 steps\n",
      "\u001b[92m Episode 30 finished after 381 steps\n",
      "\u001b[92m Episode 40 finished after 346 steps\n",
      "\u001b[99m Episode 50 finished after 144 steps\n",
      "\u001b[92m Episode 60 finished after 291 steps\n",
      "\u001b[99m Episode 70 finished after 173 steps\n",
      "\u001b[92m Episode 80 finished after 303 steps\n",
      "\u001b[92m Episode 90 finished after 500 steps\n",
      "\u001b[92m Episode 100 finished after 277 steps\n",
      "\u001b[92m Episode 110 finished after 390 steps\n",
      "\u001b[92m Episode 120 finished after 314 steps\n",
      "\u001b[92m Episode 130 finished after 257 steps\n",
      "\u001b[92m Episode 140 finished after 440 steps\n",
      "\u001b[92m Episode 150 finished after 249 steps\n",
      "\u001b[92m Episode 160 finished after 368 steps\n",
      "\u001b[92m Episode 170 finished after 291 steps\n",
      "\u001b[92m Episode 180 finished after 195 steps\n",
      "\u001b[92m Episode 190 finished after 299 steps\n",
      "\u001b[92m Episode 200 finished after 423 steps\n",
      "\u001b[92m Episode 210 finished after 463 steps\n",
      "\u001b[99m Episode 220 finished after 186 steps\n",
      "\u001b[92m Episode 230 finished after 500 steps\n",
      "\u001b[92m Episode 240 finished after 394 steps\n",
      "\u001b[92m Episode 250 finished after 282 steps\n",
      "\u001b[99m Episode 260 finished after 161 steps\n",
      "\u001b[92m Episode 270 finished after 235 steps\n",
      "\u001b[92m Episode 280 finished after 354 steps\n",
      "\u001b[99m Episode 290 finished after 188 steps\n",
      "\u001b[92m Episode 300 finished after 342 steps\n",
      "\u001b[92m Episode 310 finished after 308 steps\n",
      "\u001b[92m Episode 320 finished after 204 steps\n",
      "\u001b[92m Episode 330 finished after 339 steps\n",
      "\u001b[92m Episode 340 finished after 455 steps\n",
      "\u001b[92m Episode 350 finished after 260 steps\n",
      "\u001b[92m Episode 360 finished after 390 steps\n",
      "\u001b[92m Episode 370 finished after 237 steps\n",
      "\u001b[92m Episode 380 finished after 258 steps\n",
      "\u001b[92m Episode 390 finished after 205 steps\n",
      "\u001b[92m Episode 400 finished after 500 steps\n",
      "\u001b[92m Episode 410 finished after 204 steps\n",
      "\u001b[92m Episode 420 finished after 230 steps\n",
      "\u001b[92m Episode 430 finished after 300 steps\n",
      "\u001b[92m Episode 440 finished after 349 steps\n",
      "\u001b[92m Episode 450 finished after 500 steps\n",
      "\u001b[92m Episode 460 finished after 337 steps\n",
      "\u001b[92m Episode 470 finished after 376 steps\n",
      "\u001b[92m Episode 480 finished after 500 steps\n",
      "\u001b[92m Episode 490 finished after 276 steps\n",
      "\u001b[92m Episode 500 finished after 500 steps\n",
      "\u001b[92m Episode 510 finished after 250 steps\n",
      "\u001b[92m Episode 520 finished after 274 steps\n",
      "\u001b[92m Episode 530 finished after 218 steps\n",
      "\u001b[92m Episode 540 finished after 376 steps\n",
      "\u001b[92m Episode 550 finished after 315 steps\n",
      "\u001b[92m Episode 560 finished after 310 steps\n",
      "\u001b[99m Episode 570 finished after 155 steps\n",
      "\u001b[92m Episode 580 finished after 249 steps\n",
      "\u001b[92m Episode 590 finished after 257 steps\n",
      "\u001b[92m Episode 600 finished after 313 steps\n",
      "\u001b[92m Episode 610 finished after 296 steps\n",
      "\u001b[99m Episode 620 finished after 139 steps\n",
      "\u001b[92m Episode 630 finished after 328 steps\n",
      "\u001b[92m Episode 640 finished after 354 steps\n",
      "\u001b[92m Episode 650 finished after 259 steps\n",
      "\u001b[99m Episode 660 finished after 190 steps\n",
      "\u001b[99m Episode 670 finished after 158 steps\n",
      "\u001b[92m Episode 680 finished after 208 steps\n",
      "\u001b[92m Episode 690 finished after 226 steps\n",
      "\u001b[92m Episode 700 finished after 288 steps\n",
      "\u001b[92m Episode 710 finished after 230 steps\n",
      "\u001b[92m Episode 720 finished after 249 steps\n",
      "\u001b[92m Episode 730 finished after 284 steps\n",
      "\u001b[92m Episode 740 finished after 217 steps\n",
      "\u001b[92m Episode 750 finished after 248 steps\n",
      "\u001b[92m Episode 760 finished after 219 steps\n",
      "\u001b[92m Episode 770 finished after 204 steps\n",
      "\u001b[99m Episode 780 finished after 109 steps\n",
      "\u001b[92m Episode 790 finished after 257 steps\n",
      "\u001b[99m Episode 800 finished after 174 steps\n",
      "\u001b[92m Episode 810 finished after 274 steps\n",
      "\u001b[92m Episode 820 finished after 221 steps\n",
      "\u001b[92m Episode 830 finished after 263 steps\n",
      "\u001b[92m Episode 840 finished after 207 steps\n",
      "\u001b[99m Episode 850 finished after 142 steps\n",
      "\u001b[99m Episode 860 finished after 151 steps\n",
      "\u001b[92m Episode 870 finished after 216 steps\n",
      "\u001b[92m Episode 880 finished after 260 steps\n",
      "\u001b[92m Episode 890 finished after 202 steps\n",
      "\u001b[92m Episode 900 finished after 246 steps\n",
      "\u001b[92m Episode 910 finished after 260 steps\n",
      "\u001b[92m Episode 920 finished after 243 steps\n",
      "\u001b[92m Episode 930 finished after 302 steps\n",
      "\u001b[99m Episode 940 finished after 181 steps\n",
      "\u001b[92m Episode 950 finished after 295 steps\n",
      "\u001b[92m Episode 960 finished after 255 steps\n",
      "\u001b[92m Episode 970 finished after 227 steps\n",
      "\u001b[92m Episode 980 finished after 279 steps\n",
      "\u001b[92m Episode 990 finished after 266 steps\n",
      "it took  827.432412147522  seconds to run the program\n",
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 406 steps\n",
      "\u001b[92m Episode 20 finished after 402 steps\n",
      "\u001b[92m Episode 30 finished after 277 steps\n",
      "\u001b[92m Episode 40 finished after 454 steps\n",
      "\u001b[92m Episode 50 finished after 293 steps\n",
      "\u001b[92m Episode 60 finished after 278 steps\n",
      "\u001b[92m Episode 70 finished after 483 steps\n",
      "\u001b[92m Episode 80 finished after 285 steps\n",
      "\u001b[92m Episode 90 finished after 269 steps\n",
      "\u001b[92m Episode 100 finished after 234 steps\n",
      "\u001b[92m Episode 110 finished after 500 steps\n",
      "\u001b[92m Episode 120 finished after 409 steps\n",
      "\u001b[99m Episode 130 finished after 113 steps\n",
      "\u001b[92m Episode 140 finished after 376 steps\n",
      "\u001b[99m Episode 150 finished after 132 steps\n",
      "\u001b[92m Episode 160 finished after 254 steps\n",
      "\u001b[92m Episode 170 finished after 439 steps\n",
      "\u001b[92m Episode 180 finished after 305 steps\n",
      "\u001b[92m Episode 190 finished after 443 steps\n",
      "\u001b[92m Episode 200 finished after 500 steps\n",
      "\u001b[92m Episode 210 finished after 315 steps\n",
      "\u001b[99m Episode 220 finished after 194 steps\n",
      "\u001b[92m Episode 230 finished after 333 steps\n",
      "\u001b[92m Episode 240 finished after 284 steps\n",
      "\u001b[92m Episode 250 finished after 267 steps\n",
      "\u001b[92m Episode 260 finished after 500 steps\n",
      "\u001b[92m Episode 270 finished after 269 steps\n",
      "\u001b[92m Episode 280 finished after 500 steps\n",
      "\u001b[92m Episode 290 finished after 354 steps\n",
      "\u001b[92m Episode 300 finished after 229 steps\n",
      "\u001b[92m Episode 310 finished after 282 steps\n",
      "\u001b[92m Episode 320 finished after 216 steps\n",
      "\u001b[92m Episode 330 finished after 195 steps\n",
      "\u001b[92m Episode 340 finished after 279 steps\n",
      "\u001b[92m Episode 350 finished after 423 steps\n",
      "\u001b[92m Episode 360 finished after 426 steps\n",
      "\u001b[92m Episode 370 finished after 249 steps\n",
      "\u001b[92m Episode 380 finished after 380 steps\n",
      "\u001b[92m Episode 390 finished after 199 steps\n",
      "\u001b[99m Episode 400 finished after 146 steps\n",
      "\u001b[92m Episode 410 finished after 277 steps\n",
      "\u001b[92m Episode 420 finished after 200 steps\n",
      "\u001b[92m Episode 430 finished after 461 steps\n",
      "\u001b[92m Episode 440 finished after 318 steps\n",
      "\u001b[92m Episode 450 finished after 243 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 460 finished after 165 steps\n",
      "\u001b[99m Episode 470 finished after 141 steps\n",
      "\u001b[92m Episode 480 finished after 353 steps\n",
      "\u001b[92m Episode 490 finished after 250 steps\n",
      "\u001b[92m Episode 500 finished after 392 steps\n",
      "\u001b[92m Episode 510 finished after 220 steps\n",
      "\u001b[99m Episode 520 finished after 163 steps\n",
      "\u001b[99m Episode 530 finished after 190 steps\n",
      "\u001b[99m Episode 540 finished after 166 steps\n",
      "\u001b[92m Episode 550 finished after 203 steps\n",
      "\u001b[92m Episode 560 finished after 210 steps\n",
      "\u001b[92m Episode 570 finished after 391 steps\n",
      "\u001b[92m Episode 580 finished after 252 steps\n",
      "\u001b[92m Episode 590 finished after 338 steps\n",
      "\u001b[92m Episode 600 finished after 376 steps\n",
      "\u001b[92m Episode 610 finished after 371 steps\n",
      "\u001b[99m Episode 620 finished after 140 steps\n",
      "\u001b[92m Episode 630 finished after 302 steps\n",
      "\u001b[99m Episode 640 finished after 156 steps\n",
      "\u001b[92m Episode 650 finished after 432 steps\n",
      "\u001b[92m Episode 660 finished after 432 steps\n",
      "\u001b[92m Episode 670 finished after 301 steps\n",
      "\u001b[92m Episode 680 finished after 229 steps\n",
      "\u001b[92m Episode 690 finished after 499 steps\n",
      "\u001b[99m Episode 700 finished after 171 steps\n",
      "\u001b[92m Episode 710 finished after 342 steps\n",
      "\u001b[92m Episode 720 finished after 267 steps\n",
      "\u001b[92m Episode 730 finished after 353 steps\n",
      "\u001b[92m Episode 740 finished after 202 steps\n",
      "\u001b[92m Episode 750 finished after 360 steps\n",
      "\u001b[99m Episode 760 finished after 180 steps\n",
      "\u001b[92m Episode 770 finished after 254 steps\n",
      "\u001b[99m Episode 780 finished after 192 steps\n",
      "\u001b[92m Episode 790 finished after 233 steps\n",
      "\u001b[92m Episode 800 finished after 285 steps\n",
      "\u001b[92m Episode 810 finished after 500 steps\n",
      "\u001b[92m Episode 820 finished after 330 steps\n",
      "\u001b[92m Episode 830 finished after 287 steps\n",
      "\u001b[92m Episode 840 finished after 500 steps\n",
      "\u001b[92m Episode 850 finished after 199 steps\n",
      "\u001b[92m Episode 860 finished after 389 steps\n",
      "\u001b[92m Episode 870 finished after 212 steps\n",
      "\u001b[92m Episode 880 finished after 195 steps\n",
      "\u001b[92m Episode 890 finished after 199 steps\n",
      "\u001b[92m Episode 900 finished after 207 steps\n",
      "\u001b[92m Episode 910 finished after 359 steps\n",
      "\u001b[92m Episode 920 finished after 208 steps\n",
      "\u001b[92m Episode 930 finished after 207 steps\n",
      "\u001b[92m Episode 940 finished after 500 steps\n",
      "\u001b[92m Episode 950 finished after 272 steps\n",
      "\u001b[99m Episode 960 finished after 152 steps\n",
      "\u001b[92m Episode 970 finished after 240 steps\n",
      "\u001b[92m Episode 980 finished after 240 steps\n",
      "\u001b[92m Episode 990 finished after 349 steps\n",
      "it took  681.017226934433  seconds to run the program\n"
     ]
    }
   ],
   "source": [
    "seeds = []\n",
    "runs = 10\n",
    "num_episodes = 1000\n",
    "results = np.zeros([runs,num_episodes])\n",
    "for i in range(runs):\n",
    "    \n",
    "    batch_size = 64\n",
    "    discount_factor = 0.8\n",
    "    learn_rate = 1e-3\n",
    "    memory = ReplayMemory(10000)\n",
    "    num_hidden = 128\n",
    "    \n",
    "    seed = random.randint(1, 1000)  # This is not randomly chosen\n",
    "    seeds.append(seed)\n",
    "\n",
    "    # We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    semi_grad = False\n",
    "\n",
    "    Q_net = QNetwork(num_hidden)\n",
    "    policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "    start = time.time()\n",
    "    episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi_grad)\n",
    "    end = time.time()\n",
    "    a = np.asarray(episode_durations)\n",
    "    results[i] = a\n",
    "    print(\"it took \", end - start, \" seconds to run the program\")\n",
    "\n",
    "seedios = np.asarray(seeds)\n",
    "np.savetxt(\"fullgrad_acrobot_1000_seeds.csv\", a, delimiter=\",\")\n",
    "pd.DataFrame(results).to_csv(\"fullgrad_acrobot_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[500. 500. 149. 300. 220. 500. 179. 337. 336. 228. 167. 500. 500. 302.\n",
      "  353. 500. 500. 297. 500. 298.]\n",
      " [500. 500. 176. 155. 462. 158. 184. 154. 179. 282. 165. 232. 414. 500.\n",
      "  500. 400. 333. 380. 500. 185.]\n",
      " [500. 500. 283. 500. 407. 500. 500. 468. 500. 500. 410. 394. 206. 299.\n",
      "  395. 352. 500. 345. 411. 160.]\n",
      " [500. 432. 500. 500. 500. 299. 278. 334. 278. 336. 320. 381. 254. 190.\n",
      "  239. 257. 284. 279. 298. 418.]\n",
      " [500. 500. 500. 500. 500. 418. 500. 500. 406. 359. 317. 270. 160. 254.\n",
      "  386. 500. 500. 500. 500. 197.]\n",
      " [500. 500. 274. 196. 433. 194. 127. 258. 279. 391. 500. 378. 410. 346.\n",
      "  312. 417. 240. 314. 213. 294.]\n",
      " [500. 500. 500. 500. 500. 500. 500. 500. 468. 500. 500. 500. 500. 500.\n",
      "  500. 500. 320. 274. 500. 258.]\n",
      " [500. 500. 500. 500. 500. 242. 500. 154. 227. 259. 329. 240. 271. 232.\n",
      "  446. 500. 452. 500. 500. 500.]\n",
      " [500. 500. 157. 276. 500. 500. 500. 222. 190. 239. 218. 161. 305. 156.\n",
      "  500. 185. 138. 357. 205. 291.]\n",
      " [500. 423. 319. 259. 500. 500. 500. 257. 335. 500. 363. 500. 500. 500.\n",
      "  259. 179. 333. 324. 250. 231.]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 0 finished after 500 steps\n",
      "\u001b[92m Episode 10 finished after 500 steps\n",
      "\u001b[92m Episode 20 finished after 500 steps\n",
      "\u001b[92m Episode 30 finished after 452 steps\n",
      "\u001b[92m Episode 40 finished after 337 steps\n",
      "\u001b[92m Episode 50 finished after 248 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-59e7b494adeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedyPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mepisode_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemi_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"it took \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" seconds to run the program\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-2865682f279e>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# sample action and store in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d48c9895dfb6>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#                 print(\"before logits: \",torch.tensor(obs).float() )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#                 print(\"LOGITS: \",logits )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 1000\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "semi_grad = True\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "start = time.time()\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate, semi_grad)\n",
    "end = time.time()\n",
    "print(\"it took \", end - start, \" seconds to run the program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
